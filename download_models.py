#!/usr/bin/env python3
"""
Faster Whisper ëª¨ë¸ë“¤ì„ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import torch
from faster_whisper import WhisperModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_model(model_size: str, device: str = "auto", compute_type: str = "auto"):
    """íŠ¹ì • ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ"""
    try:
        # ì¥ì¹˜ ì„¤ì •
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # ì»´í“¨íŠ¸ íƒ€ì… ì„¤ì •
        if compute_type == "auto":
            if device == "cuda":
                compute_type = "float16"
            else:
                compute_type = "int8"
        
        logger.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_size} (device: {device}, compute_type: {compute_type})")
        
        # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë¨)
        model = WhisperModel(model_size, device=device, compute_type=compute_type)
        
        logger.info(f"âœ… ì™„ë£Œ: {model_size}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì‹¤íŒ¨: {model_size} - {str(e)}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¤ Faster Whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print("=" * 50)
    
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ì¥ì¹˜: {device.upper()}")
    if device == "cuda":
        print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB")
    
    print("\nğŸ“¦ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ë“¤:")
    
    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ë“¤ (ìš©ëŸ‰ ìˆœì„œ)
    models_to_download = [
        "tiny",      # ~39MB
        "tiny.en",   # ~39MB  
        "base",      # ~74MB
        "base.en",   # ~74MB
        "small",     # ~244MB
        "small.en",  # ~244MB
        "medium",    # ~769MB
        "medium.en", # ~769MB
        "large-v3",  # ~1550MB
        "large-v3-turbo", # ~1550MB (ë” ë¹ ë¥¸ ì²˜ë¦¬ì†ë„)
    ]
    
    # ì„ íƒì  ë‹¤ìš´ë¡œë“œ (í° ëª¨ë¸ë“¤)
    optional_models = [
        "large-v1",     # ~1550MB
        "large-v2",     # ~1550MB
        "distil-large-v2",  # ~756MB
        "distil-large-v3",  # ~756MB
    ]
    
    print("ê¸°ë³¸ ëª¨ë¸ë“¤:")
    for model in models_to_download:
        if model == "large-v3-turbo":
            print(f"  - {model} (âš¡ í„°ë³´ - ë¹ ë¥¸ ì²˜ë¦¬)")
        else:
            print(f"  - {model}")
    
    print("\nì„ íƒì  ëª¨ë¸ë“¤ (ìš©ëŸ‰ì´ í¼):")
    for model in optional_models:
        print(f"  - {model}")
    
    # ì‚¬ìš©ì ì„ íƒ
    download_all = input("\nğŸ¤” ëª¨ë“  ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()
    
    if download_all == 'y':
        all_models = models_to_download + optional_models
    else:
        all_models = models_to_download
    
    print(f"\nğŸš€ {len(all_models)}ê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    print("=" * 50)
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    success_count = 0
    failed_models = []
    
    for i, model_size in enumerate(all_models, 1):
        print(f"\n[{i}/{len(all_models)}] {model_size}")
        if model_size == "large-v3-turbo":
            print("  âš¡ í„°ë³´ ëª¨ë¸ - ë” ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„!")
        
        if download_model(model_size):
            success_count += 1
        else:
            failed_models.append(model_size)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}/{len(all_models)}")
    
    if failed_models:
        print(f"âŒ ì‹¤íŒ¨: {len(failed_models)}")
        print("ì‹¤íŒ¨í•œ ëª¨ë¸ë“¤:")
        for model in failed_models:
            print(f"  - {model}")
    
    print("\nğŸ‰ ëª¨ë“  ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ’¡ ì¶”ì²œ: large-v3-turbo ëª¨ë¸ì€ ì •í™•ë„ëŠ” large-v3ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë” ë¹ ë¦…ë‹ˆë‹¤!")
    print("ì´ì œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¹ ë¥´ê²Œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 